{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber\n",
        "!pip install spacy\n",
        "!pip install nltk\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18rxbxjnvsjc",
        "outputId": "85c3bb83-ed4d-49c8-882d-91dc345fba75"
      },
      "id": "18rxbxjnvsjc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (11.0.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.5 pypdfium2-4.30.1\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdKU2j5zPEfh",
        "outputId": "01019c45-8760-4c9b-b33b-c6ed4872e6d1"
      },
      "id": "LdKU2j5zPEfh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78cabe05-c3db-4a55-af41-48da02915a03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78cabe05-c3db-4a55-af41-48da02915a03",
        "outputId": "a220c82c-0ff8-4df9-85b7-d9960838b604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import pdfplumber\n",
        "import re\n",
        "import spacy\n",
        "import datetime\n",
        "import nltk\n",
        "import os\n",
        "import pandas as pd\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Load SpaCy for skills extraction\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Download necessary NLTK datasets\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def parse_resumes(folder_path, output_file):\n",
        "    def extract_text_from_pdf(pdf_path):\n",
        "        extracted_text = \"\"\n",
        "        try:\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                for page in pdf.pages:\n",
        "                    extracted_text += page.extract_text()\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading PDF: {e}\")\n",
        "        return extracted_text\n",
        "\n",
        "    def preprocess_text(text):\n",
        "        return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    def extract_contact_info(text):\n",
        "        email_regex = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "        phone_regex = r'\\b\\d{10}\\b|\\b(\\+?\\d{1,3}[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b'\n",
        "        github_regex = r'https://github.com/[A-Za-z0-9_-]+'\n",
        "\n",
        "        email = re.search(email_regex, text)\n",
        "        phone = re.search(phone_regex, text)\n",
        "        github = re.search(github_regex, text)\n",
        "\n",
        "        return {\n",
        "            \"email\": email.group() if email else None,\n",
        "            \"phone\": phone.group() if phone else None,\n",
        "            \"github\": github.group() if github else None\n",
        "        }\n",
        "\n",
        "    def extract_skills(text):\n",
        "        skill_keywords = ['Python', 'Machine Learning', 'NLP', 'Data Analysis', 'Java', 'SQL',\n",
        "        'C++', 'AWS', 'Docker', 'TensorFlow', 'Keras', 'React', 'Node.js']\n",
        "        doc = nlp(text)\n",
        "\n",
        "        extracted_skills = set()\n",
        "        for token in doc:\n",
        "            if token.text in skill_keywords:\n",
        "                extracted_skills.add(token.text)\n",
        "        return list(extracted_skills)\n",
        "\n",
        "    def extract_experience(text):\n",
        "        experience_patterns = [\n",
        "            r'(\\d+(?:\\.\\d+)?)\\s*(?:years?|yrs?)(?:\\s*of)?\\s*(?:total\\s*)?(?:work\\s*)?experience',\n",
        "            r'total\\s*experience[:\\s](\\d+(?:\\.\\d+)?)\\s(?:years?|yrs?)',\n",
        "            r'(\\d+(?:\\.\\d+)?)\\s*(?:years?|yrs?)\\s*(?:of\\s*)?(?:professional\\s*)?experience',\n",
        "            r'work\\s*experience[:\\s](\\d+(?:\\.\\d+)?)\\s(?:years?|yrs?)',\n",
        "            r'(\\d+(?:\\.\\d+)?)\\s*(?:year|years)\\s*(?:of\\s*)?(?:total\\s*)?experience'\n",
        "        ]\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        for pattern in experience_patterns:\n",
        "            match = re.search(pattern, text_lower)\n",
        "            if match:\n",
        "                try:\n",
        "                    return float(match.group(1))\n",
        "                except (ValueError, TypeError):\n",
        "                    continue\n",
        "\n",
        "        job_experience_pattern = r'(?:from|)\\s*(\\d{4})\\s*(?:to|[-])\\s*(?:present|current|(\\d{4}))'\n",
        "        job_dates = re.findall(job_experience_pattern, text_lower)\n",
        "\n",
        "        if job_dates:\n",
        "            current_year = datetime.datetime.now().year\n",
        "            total_years = 0\n",
        "\n",
        "            for start, end in job_dates:\n",
        "                start_year = int(start)\n",
        "                end_year = int(end) if end else current_year\n",
        "                total_years += end_year - start_year\n",
        "\n",
        "            return total_years\n",
        "\n",
        "        return \"0\"\n",
        "\n",
        "    def extract_promotions(text):\n",
        "        promotion_keywords = [\n",
        "            r'promoted', r'promotion', r'lead', r'senior', r'head', r'manager', r'director', r'chief'\n",
        "        ]\n",
        "\n",
        "        promotion_count = 0\n",
        "        for keyword in promotion_keywords:\n",
        "            promotion_count += len(re.findall(keyword, text.lower()))\n",
        "\n",
        "        return promotion_count\n",
        "\n",
        "    def get_name(text):\n",
        "        sentences = sent_tokenize(text)\n",
        "        for sentence in sentences[:5]:\n",
        "            words = word_tokenize(sentence)\n",
        "            pos_tags = pos_tag(words)\n",
        "            for i, (word, tag) in enumerate(pos_tags):\n",
        "                if tag == 'NNP':\n",
        "                    return word\n",
        "        return \"Unknown\"\n",
        "\n",
        "    resume_data = []\n",
        "    unique_id = 1\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.pdf'):\n",
        "            pdf_path = os.path.join(folder_path, filename)\n",
        "            print(f\"Processing {filename}\")\n",
        "\n",
        "            raw_text = extract_text_from_pdf(pdf_path)\n",
        "            if raw_text:\n",
        "                cleaned_text = preprocess_text(raw_text)\n",
        "\n",
        "                contact_info = extract_contact_info(cleaned_text)\n",
        "                skills = extract_skills(cleaned_text)\n",
        "                experience_years = extract_experience(cleaned_text)\n",
        "                promotions = extract_promotions(cleaned_text)\n",
        "\n",
        "                resume_data.append({\n",
        "                    \"unique_id\": unique_id,\n",
        "                    \"name\": get_name(cleaned_text),\n",
        "                    \"email\": contact_info[\"email\"],\n",
        "                    \"phone\": contact_info[\"phone\"],\n",
        "                    \"github\": contact_info[\"github\"],\n",
        "                    \"experience_years\": experience_years,\n",
        "                    \"skills\": \", \".join(skills),\n",
        "                    \"promotions\": promotions\n",
        "                })\n",
        "\n",
        "                unique_id += 1\n",
        "\n",
        "    df = pd.DataFrame(resume_data)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Data saved to {output_file}\")\n",
        "\n",
        "# Example usage:\n",
        "# parse_resumes(\"/content/resumes\", \"/content/resumes/extracted_data_with_promotions.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d61f805e-cb2d-46ce-b287-42b9d8fe330a",
      "metadata": {
        "id": "d61f805e-cb2d-46ce-b287-42b9d8fe330a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80909b12-4faa-4176-83e0-d5dec299e41b",
      "metadata": {
        "id": "80909b12-4faa-4176-83e0-d5dec299e41b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cf5c22a-dc20-4a7b-88a8-cafd8d424f9c",
      "metadata": {
        "id": "3cf5c22a-dc20-4a7b-88a8-cafd8d424f9c"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "def analyse_portfolio(csv_path, github_token, output_file_path, experience_weight=0.6, skills_weight=0.4):\n",
        "    \"\"\"\n",
        "    Analyse a portfolio based on GitHub links, skills, and experience, and calculate technical expertise scores.\n",
        "\n",
        "    Args:\n",
        "        csv_path (str): Path to the input CSV file.\n",
        "        github_token (str): GitHub API authentication token.\n",
        "        output_file_path (str): Path to save the updated CSV file.\n",
        "        experience_weight (float): Weight to assign to experience in dynamic scoring.\n",
        "        skills_weight (float): Weight to assign to skills in dynamic scoring.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: Updated DataFrame with technical expertise scores.\n",
        "    \"\"\"\n",
        "\n",
        "    def extract_github_features(github_url, token):\n",
        "        username = github_url.rstrip(\"/\").split(\"/\")[-1]\n",
        "        headers = {\"Authorization\": f\"token {token}\"}\n",
        "\n",
        "        user_url = f\"https://api.github.com/users/{username}\"\n",
        "        repos_url = f\"https://api.github.com/users/{username}/repos\"\n",
        "\n",
        "        try:\n",
        "            user_response = requests.get(user_url, headers=headers)\n",
        "            user_data = user_response.json()\n",
        "\n",
        "            repos_response = requests.get(repos_url, headers=headers)\n",
        "            repos_data = repos_response.json()\n",
        "\n",
        "            features = {\n",
        "                \"username\": user_data.get(\"login\"),\n",
        "                \"name\": user_data.get(\"name\"),\n",
        "                \"public_repos_count\": user_data.get(\"public_repos\", 0),\n",
        "                \"top_repositories\": [],\n",
        "                \"languages\": [],\n",
        "                \"commit_count\": 0,\n",
        "                \"community_interaction\": 0\n",
        "            }\n",
        "\n",
        "            for repo in repos_data:\n",
        "                if not repo:\n",
        "                    continue\n",
        "                repo_details = {\n",
        "                    \"name\": repo.get(\"name\"),\n",
        "                    \"stars\": repo.get(\"stargazers_count\", 0),\n",
        "                    \"forks\": repo.get(\"forks_count\", 0),\n",
        "                    \"language\": repo.get(\"language\")\n",
        "                }\n",
        "                if repo.get(\"language\") == \"Jupyter Notebook\":\n",
        "                    repo_details[\"language\"] = \"Python\"\n",
        "\n",
        "                features[\"top_repositories\"].append(repo_details)\n",
        "                if repo.get(\"language\"):\n",
        "                    features[\"languages\"].append(repo.get(\"language\"))\n",
        "                features[\"commit_count\"] += repo.get(\"stargazers_count\", 0)\n",
        "                if repo.get(\"pulls_url\"):\n",
        "                    features[\"community_interaction\"] += 1\n",
        "\n",
        "            features[\"top_repositories\"] = sorted(\n",
        "                features[\"top_repositories\"], key=lambda x: x[\"stars\"], reverse=True\n",
        "            )[:5]\n",
        "\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {github_url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def calculate_technical_score(features):\n",
        "        if not features or not features.get(\"top_repositories\"):\n",
        "            return 0\n",
        "\n",
        "        code_quality_score = 0.80\n",
        "        tech_stack_score = min(1, len(set(features[\"languages\"])) / 10)\n",
        "        project_complexity_score = sum([repo[\"stars\"] for repo in features[\"top_repositories\"]]) / 500\n",
        "        project_complexity_score = min(project_complexity_score, 1)\n",
        "        contribution_score = features[\"community_interaction\"] / 10\n",
        "        testing_doc_score = 0.90\n",
        "\n",
        "        technical_score = (\n",
        "            (code_quality_score * 0.30) +\n",
        "            (tech_stack_score * 0.25) +\n",
        "            (project_complexity_score * 0.25) +\n",
        "            (contribution_score * 0.10) +\n",
        "            (testing_doc_score * 0.10)\n",
        "        )\n",
        "\n",
        "        return round(technical_score * 100, 2)\n",
        "\n",
        "    def calculate_skills_score(skills_list):\n",
        "        if not skills_list:\n",
        "            return 0.0\n",
        "        return min(len(skills_list) * 10, 100.0)\n",
        "\n",
        "    def calculate_experience_score(years_of_experience):\n",
        "        if not years_of_experience:\n",
        "            return 0.0\n",
        "        return min(years_of_experience * 10, 100.0)\n",
        "\n",
        "    def calculate_dynamic_score(row):\n",
        "        skills_score = calculate_skills_score(row['skills'].split(','))  # assuming skills are comma-separated\n",
        "        experience_score = calculate_experience_score(row['experience_years'])\n",
        "        dynamic_score = (skills_score * skills_weight) + (experience_score * experience_weight)\n",
        "        return dynamic_score\n",
        "\n",
        "    def process_github_links(df):\n",
        "        github_column = 'github'\n",
        "\n",
        "        if github_column not in df.columns:\n",
        "            print(f\"Error: No column named '{github_column}' found in the CSV.\")\n",
        "            return None\n",
        "\n",
        "        df['technical_expertise_score'] = 0.0\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            github_url = row[github_column]\n",
        "            if pd.notna(github_url):\n",
        "                try:\n",
        "                    features = extract_github_features(github_url, github_token)\n",
        "                    if features:\n",
        "                        technical_score = calculate_technical_score(features)\n",
        "                        df.at[index, 'technical_expertise_score'] = technical_score\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {github_url}: {e}\")\n",
        "                    df.at[index, 'technical_expertise_score'] = 0.0\n",
        "            else:\n",
        "                dynamic_score = calculate_dynamic_score(row)\n",
        "                df.at[index, 'technical_expertise_score'] = dynamic_score\n",
        "\n",
        "        return df\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV file: {e}\")\n",
        "        return None\n",
        "\n",
        "    updated_df = process_github_links(df)\n",
        "\n",
        "    if updated_df is not None:\n",
        "        updated_df.to_csv(output_file_path, index=False)\n",
        "        print(f\"Analysis complete. Results saved to {output_file_path}\")\n",
        "        print(\"\\nTop 5 Profiles by Technical Expertise Score:\")\n",
        "        top_profiles = updated_df.nlargest(5, 'technical_expertise_score')\n",
        "        print(top_profiles[['github', 'technical_expertise_score']])\n",
        "\n",
        "    return updated_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36db302c-e02b-4751-99c8-9d9c34ebc8ef",
      "metadata": {
        "id": "36db302c-e02b-4751-99c8-9d9c34ebc8ef"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d4d45b0-9def-4e8d-a7a0-2c567464fcca",
      "metadata": {
        "id": "9d4d45b0-9def-4e8d-a7a0-2c567464fcca"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "842b810c-1547-4f03-9f5a-3950fd3cf9d3",
      "metadata": {
        "id": "842b810c-1547-4f03-9f5a-3950fd3cf9d3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN, Dense, Input\n",
        "\n",
        "def career_trajectory(csv_path, target_column, output_file_path):\n",
        "    \"\"\"\n",
        "    Analyzes career trajectory based on experience, promotions, and technical expertise score.\n",
        "\n",
        "    Args:\n",
        "        csv_path (str): Path to the input CSV file.\n",
        "        target_column (str): Column name for the target variable (e.g., technical_expertise_score).\n",
        "        output_file_path (str): Path to save the updated CSV file.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: Updated DataFrame with predicted growth values.\n",
        "    \"\"\"\n",
        "    # Suppress TensorFlow warnings\n",
        "    tf.get_logger().setLevel('ERROR')\n",
        "    warnings.filterwarnings('ignore', category=UserWarning, module='keras')\n",
        "\n",
        "    try:\n",
        "        # Load dataset\n",
        "        df = pd.read_csv(csv_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV file: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Ensure column names are cleaned\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "    try:\n",
        "        # Extract relevant columns\n",
        "        X = df[['experience_years', 'promotions']].values\n",
        "        y = df[target_column].values\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: One or more required columns are missing from the dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Reshape X for RNN input\n",
        "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "    # Define RNN model\n",
        "    model = Sequential([\n",
        "        Input(shape=(X.shape[1], 1)),\n",
        "        SimpleRNN(64, activation='relu', return_sequences=False),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X, y, epochs=50, batch_size=1, verbose=0)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(X)\n",
        "\n",
        "    # Add predictions to DataFrame\n",
        "    df['predicted_growth'] = predictions\n",
        "\n",
        "    # Save the updated DataFrame\n",
        "    df.to_csv(output_file_path, index=False)\n",
        "\n",
        "    # Display predictions\n",
        "    table = df[['unique_id', 'name', 'experience_years', 'promotions', target_column, 'predicted_growth']]\n",
        "    print(\"\\nPredictions for All Candidates:\")\n",
        "    print(table.to_string(index=False))\n",
        "\n",
        "    # Visualize the results\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    # Plot 1: Years of Experience vs Predicted Growth\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.scatterplot(\n",
        "        x=df['experience_years'], y=df['predicted_growth'], hue=df['name'],\n",
        "        palette='Set2', s=100, edgecolor='black'\n",
        "    )\n",
        "    plt.title('Years of Experience vs Predicted Growth')\n",
        "    plt.xlabel('Years of Experience')\n",
        "    plt.ylabel('Predicted Growth')\n",
        "    plt.legend(title='Name', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "    # Plot 2: Number of Promotions vs Predicted Growth\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.scatterplot(\n",
        "        x=df['promotions'], y=df['predicted_growth'], hue=df['name'],\n",
        "        palette='Set2', s=100, edgecolor='black'\n",
        "    )\n",
        "    plt.title('Number of Promotions vs Predicted Growth')\n",
        "    plt.xlabel('Number of Promotions')\n",
        "    plt.ylabel('Predicted Growth')\n",
        "    plt.legend(title='Name', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "    # Show plots\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3564b102-5401-4d80-837b-a59e78c3065e",
      "metadata": {
        "id": "3564b102-5401-4d80-837b-a59e78c3065e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47c45c24-e5c1-47d1-8263-a417a0cce77d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47c45c24-e5c1-47d1-8263-a417a0cce77d",
        "outputId": "e2f245f4-e05e-4de6-bcfb-65b9112ebeae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.11.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.12.14)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b817b8be-aa5e-4c2a-be20-f5e2fa903ad6",
      "metadata": {
        "id": "b817b8be-aa5e-4c2a-be20-f5e2fa903ad6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "\n",
        "def GNN_module(resume_data, job_data):\n",
        "    \"\"\"\n",
        "    Constructs and trains a Graph Neural Network (GNN) model to predict compatibility scores between resumes and job postings.\n",
        "\n",
        "    Args:\n",
        "        resume_data (list of list): List of parsed resume feature vectors.\n",
        "        job_data (list of list): List of job feature vectors.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Validate input data\n",
        "    if not resume_data or not job_data:\n",
        "        raise ValueError(\"Both resume_data and job_data must be provided and non-empty.\")\n",
        "\n",
        "    # Convert inputs to numpy arrays\n",
        "    resume_features = np.array(resume_data)\n",
        "    job_features = np.array(job_data)\n",
        "\n",
        "    n_resumes = resume_features.shape[0]\n",
        "    n_jobs = job_features.shape[0]\n",
        "\n",
        "    # Compatibility heuristic with nuanced scoring (weighted dot product)\n",
        "    edge_weights = np.zeros((n_resumes, n_jobs))\n",
        "\n",
        "    for i in range(n_resumes):\n",
        "        for j in range(n_jobs):\n",
        "            edge_weights[i, j] = np.dot(resume_features[i], job_features[j])  # Weighted match\n",
        "\n",
        "    # Step 2: Create Bipartite Graph\n",
        "    graph_edges = []\n",
        "    edge_weights_list = []\n",
        "\n",
        "    for i in range(n_resumes):\n",
        "        for j in range(n_jobs):\n",
        "            graph_edges.append((i, n_resumes + j))  # Bipartite edges\n",
        "            edge_weights_list.append(edge_weights[i, j])\n",
        "\n",
        "    edge_index = torch.tensor(graph_edges, dtype=torch.long).t().contiguous()\n",
        "    edge_attr = torch.tensor(edge_weights_list, dtype=torch.float)\n",
        "\n",
        "    # Node features\n",
        "    x_resumes = torch.tensor(resume_features, dtype=torch.float)\n",
        "    x_jobs = torch.tensor(job_features, dtype=torch.float)\n",
        "    x = torch.cat([x_resumes, x_jobs], dim=0)\n",
        "\n",
        "    # Step 3: Adjusted GNN Model for Edge Predictions\n",
        "    class GNNModel(nn.Module):\n",
        "        def __init__(self, input_dim, hidden_dim):\n",
        "            super(GNNModel, self).__init__()\n",
        "            self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "            self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "            self.fc_edge = nn.Linear(2 * hidden_dim, 1)  # For edge-level predictions\n",
        "\n",
        "        def forward(self, x, edge_index):\n",
        "            x = self.conv1(x, edge_index)\n",
        "            x = torch.relu(x)\n",
        "            x = self.conv2(x, edge_index)\n",
        "            x = torch.relu(x)\n",
        "\n",
        "            # Generate edge-level embeddings\n",
        "            edge_src, edge_tgt = edge_index\n",
        "            edge_embeddings = torch.cat([x[edge_src], x[edge_tgt]], dim=-1)\n",
        "            edge_scores = self.fc_edge(edge_embeddings)\n",
        "            return edge_scores\n",
        "\n",
        "    # Step 4: Updated Model and Data\n",
        "    model = GNNModel(input_dim=resume_features.shape[1], hidden_dim=16)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Training and Testing Split\n",
        "    num_edges = edge_index.size(1)\n",
        "    train_edges, test_edges = train_test_split(range(num_edges), test_size=0.2, random_state=42)\n",
        "    train_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
        "    train_mask[train_edges] = True\n",
        "\n",
        "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
        "\n",
        "    def train():\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index).squeeze()  # Get edge predictions\n",
        "        train_loss = criterion(out[train_mask], data.edge_attr[train_mask])\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        return train_loss.item()\n",
        "\n",
        "    def test():\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            out = model(data.x, data.edge_index).squeeze()  # Get edge predictions\n",
        "            test_loss = criterion(out[~train_mask], data.edge_attr[~train_mask])\n",
        "        return test_loss.item()\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(50):\n",
        "        train_loss = train()\n",
        "        test_loss = test()\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "    # Step 5: Visualization\n",
        "    final_scores = model(data.x, data.edge_index).squeeze().detach().numpy()\n",
        "\n",
        "    B = nx.Graph()\n",
        "\n",
        "    # Add nodes\n",
        "    for i in range(n_resumes):\n",
        "        B.add_node(i, bipartite=0, label=f\"Resume {i+1}\")\n",
        "    for j in range(n_jobs):\n",
        "        B.add_node(n_resumes + j, bipartite=1, label=f\"Job {j+1}\")\n",
        "\n",
        "    # Add edges with final scores as weights\n",
        "    for idx, (src, tgt) in enumerate(graph_edges):\n",
        "        B.add_edge(src, tgt, weight=final_scores[idx])\n",
        "\n",
        "    pos = nx.drawing.layout.bipartite_layout(B, nodes=[i for i in range(n_resumes)])\n",
        "    nx.draw(B, pos, with_labels=True, node_color=\"lightblue\")\n",
        "    labels = nx.get_edge_attributes(B, 'weight')\n",
        "    nx.draw_networkx_edge_labels(B, pos, edge_labels={k: f\"{v:.2f}\" for k, v in labels.items()})\n",
        "    plt.title(\"Bipartite Graph with Compatibility Scores\")\n",
        "    plt.show()\n",
        "\n",
        "# Example Usage:\n",
        "# parsed_resumes = [[0.6, 0.8, 0.9], [0.5, 0.7, 0.6], ...]\n",
        "# parsed_jobs = [[0.7, 0.2, 0.1], [0.4, 0.4, 0.2], ...]\n",
        "# GNN_module(parsed_resumes, parsed_jobs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44fa7bc4-da58-486a-949c-c24ac02c6a10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44fa7bc4-da58-486a-949c-c24ac02c6a10",
        "outputId": "1850f8a3-0172-44c1-8f79-4a03490dc26b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.39.0-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: numpy<2.1 in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.4.2)\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\n",
            "  Downloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.7.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.5.0)\n",
            "Collecting pennylane-lightning>=0.39 (from pennylane)\n",
            "  Downloading PennyLane_Lightning-0.39.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pennylane) (24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2024.12.14)\n",
            "Downloading PennyLane-0.39.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.7.0-py3-none-any.whl (930 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.0/930.0 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PennyLane_Lightning-0.39.0-cp310-cp310-manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: appdirs, rustworkx, autoray, pennylane-lightning, pennylane\n",
            "Successfully installed appdirs-1.4.4 autoray-0.7.0 pennylane-0.39.0 pennylane-lightning-0.39.0 rustworkx-0.15.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pennylane"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d9fe5af-785d-4009-afea-d63b76671f48",
      "metadata": {
        "id": "3d9fe5af-785d-4009-afea-d63b76671f48"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import pennylane as qml\n",
        "from pennylane.optimize import NesterovMomentumOptimizer\n",
        "from scipy.sparse import lil_matrix\n",
        "\n",
        "\n",
        "def quantum_matching(resumes, jobs):\n",
        "    \"\"\"\n",
        "    Quantum-inspired matching algorithm to pair resumes with job descriptions.\n",
        "\n",
        "    Args:\n",
        "        resumes (list of str): List of resumes as text.\n",
        "        jobs (list of str): List of job descriptions as text.\n",
        "\n",
        "    Returns:\n",
        "        dict: Mapping of resume indices to assigned job indices.\n",
        "    \"\"\"\n",
        "    # Step 1: Preprocess and compute compatibility scores using TF-IDF\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    all_text = resumes + jobs\n",
        "\n",
        "    # Fit the vectorizer and transform the text data\n",
        "    text_vectors = vectorizer.fit_transform(all_text)\n",
        "    resume_vectors = text_vectors[:len(resumes)]\n",
        "    job_vectors = text_vectors[len(resumes):]\n",
        "\n",
        "    # Apply dimensionality reduction using SVD\n",
        "    n_components = min(50, resume_vectors.shape[1])\n",
        "    svd = TruncatedSVD(n_components=n_components)\n",
        "    reduced_resume_vectors = svd.fit_transform(resume_vectors)\n",
        "    reduced_job_vectors = svd.transform(job_vectors)\n",
        "\n",
        "    # Compute cosine similarity matrix\n",
        "    compatibility_matrix = cosine_similarity(reduced_resume_vectors, reduced_job_vectors)\n",
        "\n",
        "    # Step 2: Frame the problem as a QUBO\n",
        "    lambda_constraint = 10  # Penalty weight for constraints\n",
        "    n_candidates = len(resumes)\n",
        "    n_jobs = len(jobs)\n",
        "\n",
        "    qubo_matrix = lil_matrix((n_candidates * n_jobs, n_candidates * n_jobs))\n",
        "\n",
        "    for i in range(n_candidates):\n",
        "        for j in range(n_jobs):\n",
        "            idx = i * n_jobs + j\n",
        "            qubo_matrix[idx, idx] = -compatibility_matrix[i, j]\n",
        "\n",
        "    for i in range(n_candidates):\n",
        "        for j1 in range(n_jobs):\n",
        "            for j2 in range(j1 + 1, n_jobs):\n",
        "                idx1 = i * n_jobs + j1\n",
        "                idx2 = i * n_jobs + j2\n",
        "                qubo_matrix[idx1, idx2] += lambda_constraint\n",
        "                qubo_matrix[idx2, idx1] += lambda_constraint\n",
        "\n",
        "    for j in range(n_jobs):\n",
        "        for i1 in range(n_candidates):\n",
        "            for i2 in range(i1 + 1, n_candidates):\n",
        "                idx1 = i1 * n_jobs + j\n",
        "                idx2 = i2 * n_jobs + j\n",
        "                qubo_matrix[idx1, idx2] += lambda_constraint\n",
        "                qubo_matrix[idx2, idx1] += lambda_constraint\n",
        "\n",
        "    # Step 3: Quantum Annealing Optimization\n",
        "    def cost_fn(params):\n",
        "        binary_vector = (params > 0.5).astype(int)\n",
        "        return binary_vector @ qubo_matrix @ binary_vector.T\n",
        "\n",
        "    opt = NesterovMomentumOptimizer(stepsize=0.1)\n",
        "    params = np.random.rand(n_candidates * n_jobs)\n",
        "\n",
        "    for _ in range(100):\n",
        "        params = opt.step(cost_fn, params)\n",
        "\n",
        "    binary_vector = (params > 0.5).astype(int)\n",
        "    binary_assignment = binary_vector.reshape((n_candidates, n_jobs))\n",
        "\n",
        "    # Step 4: Greedy Refinement\n",
        "    final_assignment = np.zeros_like(binary_assignment)\n",
        "    while np.sum(final_assignment) < min(n_candidates, n_jobs):\n",
        "        max_indices = np.unravel_index(np.argmax(compatibility_matrix * binary_assignment, axis=None), compatibility_matrix.shape)\n",
        "        candidate_idx, job_idx = max_indices\n",
        "        final_assignment[candidate_idx, job_idx] = 1\n",
        "        binary_assignment[candidate_idx, :] = 0\n",
        "        binary_assignment[:, job_idx] = 0\n",
        "\n",
        "    # Step 5: Interpret Results\n",
        "    assignment = {}\n",
        "    for candidate_idx, assignment_row in enumerate(final_assignment):\n",
        "        assigned_jobs = np.where(assignment_row == 1)[0]\n",
        "        if assigned_jobs.size > 0:\n",
        "            job_idx = assigned_jobs[0]\n",
        "            assignment[candidate_idx] = job_idx\n",
        "        else:\n",
        "            assignment[candidate_idx] = None\n",
        "\n",
        "    return assignment\n",
        "\n",
        "# Example Usage:\n",
        "# resumes = [\"Experienced data scientist with expertise in Python and machine learning.\", ...]\n",
        "# jobs = [\"Looking for a data scientist skilled in Python and ML techniques.\", ...]\n",
        "# print(quantum_matching(resumes, jobs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a592991d-f5d8-46f5-9698-619995fc0415",
      "metadata": {
        "id": "a592991d-f5d8-46f5-9698-619995fc0415"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "658898de-e01b-4f41-adec-658c80fc67ee",
      "metadata": {
        "id": "658898de-e01b-4f41-adec-658c80fc67ee"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c21a2d1-e7ac-4d66-97b0-312948a16b28",
      "metadata": {
        "id": "1c21a2d1-e7ac-4d66-97b0-312948a16b28"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e92255c3-46fa-4b48-8319-d7627eda04de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e92255c3-46fa-4b48-8319-d7627eda04de",
        "outputId": "99e48aad-db83-4719-c4d6-e70b749d8a25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Download the necessary NLTK data package\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bce4adea-7eb1-45fc-ba19-ca208547e57a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bce4adea-7eb1-45fc-ba19-ca208547e57a",
        "outputId": "fbd369c5-62c2-4dd5-8820-4c1ed41f103f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.5)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (11.0.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Example Usage:\n",
        "!pip install pdfplumber\n",
        "!pip install spacy\n",
        "!pip install nltk\n",
        "!python -m spacy download en_core_web_sm\n",
        "# %%\n",
        "import pdfplumber\n",
        "import re\n",
        "import spacy\n",
        "import datetime\n",
        "import nltk\n",
        "import os\n",
        "import pandas as pd\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Load SpaCy for skills extraction\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Download necessary NLTK datasets\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab') # This line has been added\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gnn_and_generate_csv(resume_file, job_file, output_file, epochs=50, learning_rate=0.01):\n",
        "    resumes = pd.read_csv(resume_file)\n",
        "    job_descriptions = pd.read_csv(job_file)\n",
        "\n",
        "    # Preprocess skills\n",
        "    resumes['skills'] = resumes['skills'].apply(lambda x: safe_eval_skills(x) if isinstance(x, str) else x)\n",
        "    job_descriptions['skills'] = job_descriptions['skills'].apply(lambda x: safe_eval_skills(x) if isinstance(x, str) else x)\n",
        "\n",
        "    # Vectorize skills using TF-IDF\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    resume_skills_text = resumes['skills'].apply(lambda x: ', '.join(x) if isinstance(x, list) else '')\n",
        "    job_skills_text = job_descriptions['skills'].apply(lambda x: ', '.join(x) if isinstance(x, list) else '')\n",
        "\n",
        "    resume_skill_vectors = vectorizer.fit_transform(resume_skills_text)\n",
        "    job_skill_vectors = vectorizer.transform(job_skills_text)\n",
        "\n",
        "    # Compute initial compatibility scores\n",
        "    initial_scores = cosine_similarity(resume_skill_vectors, job_skill_vectors)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    resume_features = torch.tensor(resume_skill_vectors.toarray(), dtype=torch.float32)\n",
        "    job_features = torch.tensor(job_skill_vectors.toarray(), dtype=torch.float32)\n",
        "    node_features = torch.cat([resume_features, job_features], dim=0)\n",
        "\n",
        "    # Create edge list and weights\n",
        "    num_resumes = resume_features.size(0)\n",
        "    num_jobs = job_features.size(0)\n",
        "\n",
        "    edges_src, edges_dst, edge_weights = [], [], []\n",
        "    for i in range(num_resumes):\n",
        "        for j in range(num_jobs):\n",
        "            edges_src.append(i)\n",
        "            edges_dst.append(num_resumes + j)\n",
        "            edge_weights.append(initial_scores[i, j])\n",
        "\n",
        "    edge_index = torch.tensor([edges_src, edges_dst], dtype=torch.long)\n",
        "    edge_weights = torch.tensor(edge_weights, dtype=torch.float32)\n",
        "\n",
        "    # Create graph data\n",
        "    data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_weights)\n",
        "    labels = (edge_weights > 0.5).float()\n",
        "\n",
        "    # Initialize the GNN model\n",
        "    model = CompatibilityGNN(in_feats=node_features.size(1), hidden_feats=64, out_feats=32)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index)\n",
        "        edge_src, edge_dst = data.edge_index\n",
        "        edge_logits = (out[edge_src] * out[edge_dst]).sum(dim=1)\n",
        "        loss = loss_fn(edge_logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Compatibility scores post-training\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data.x, data.edge_index)\n",
        "        edge_src, edge_dst = data.edge_index\n",
        "        compatibility_scores = (out[edge_src] * out[edge_dst]).sum(dim=1).sigmoid()\n",
        "\n",
        "    # Check for the presence of 'id' columns or fallback to index\n",
        "    resume_ids = resumes['id'] if 'id' in resumes.columns else resumes.index\n",
        "    job_ids = job_descriptions['id'] if 'id' in job_descriptions.columns else job_descriptions.index\n",
        "\n",
        "    # Prepare results for CSV\n",
        "    results = []\n",
        "    for i in range(num_resumes):\n",
        "        for j in range(num_jobs):\n",
        "            score = compatibility_scores[i * num_jobs + j].item()\n",
        "            results.append({\n",
        "                'Resume_ID': resume_ids[i],\n",
        "                'Job_ID': job_ids[j],\n",
        "                'Compatibility_Score': score\n",
        "            })\n",
        "\n",
        "    # Save results to a CSV file\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv(output_file, index=False)\n",
        "    print(f\"Compatibility scores saved to {output_file}\")\n",
        "\n",
        "# Example usage\n",
        "# Replace with actual file paths and output file\n",
        "\n"
      ],
      "metadata": {
        "id": "6f3IUtTlAp-N"
      },
      "id": "6f3IUtTlAp-N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73f01580-9245-44b3-9bbd-87f60245e560",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "73f01580-9245-44b3-9bbd-87f60245e560",
        "outputId": "77340b0c-0c5e-488e-bdf9-53455dd5b100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing entry-level-data-analyst-resume-example (2).pdf\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-67f5c90a223d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfolder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/resume\"\u001b[0m  \u001b[0;31m# Input folder containing resumes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moutput_parsed_resumes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"parsed_resumes.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mparse_resumes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_parsed_resumes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/resume/p_r.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Step 2: Analyze GitHub portfolio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-c01b43e52312>\u001b[0m in \u001b[0;36mparse_resumes\u001b[0;34m(folder_path, output_file)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 resume_data.append({\n\u001b[1;32m    136\u001b[0m                     \u001b[0;34m\"unique_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0munique_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                     \u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m                     \u001b[0;34m\"email\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontact_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"email\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                     \u001b[0;34m\"phone\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontact_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"phone\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-c01b43e52312>\u001b[0m in \u001b[0;36mget_name\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mpos_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'NNP'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \"\"\"\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load, lang)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_conf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_tagdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mload_from_json\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# Automatically find path to the tagger if location is not specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"taggers/averaged_perceptron_tagger_{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTAGGER_JSONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Parse resumes\n",
        "folder_path = \"/content/resume\"  # Input folder containing resumes\n",
        "output_parsed_resumes = \"parsed_resumes.csv\"\n",
        "parse_resumes(folder_path, output_parsed_resumes)\n",
        "out=\"/content/resume/p_r.csv\"\n",
        "# Step 2: Analyze GitHub portfolio\n",
        "csv_path = output_parsed_resumes  # Input parsed resumes\n",
        "portfolio_output = \"portfolio_analysis.csv\"\n",
        "github_token = \"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\"\n",
        "analyse_portfolio(csv_path, github_token, portfolio_output, experience_weight=0.6, skills_weight=0.4)\n",
        "\n",
        "# Step 3: Analyze career trajectory\n",
        "career_trajectory_output = \"career_trajectory_analysis.csv\"\n",
        "career_trajectory(csv_path, target_column=\"promotions\", output_file_path=career_trajectory_output)\n",
        "\n",
        "# Step 4: GNN module for resume-job matching\n",
        "# Load processed resume and job data\n",
        "resume_data = portfolio_output  # Assuming this contains processed data\n",
        "job_data = \"/content/sample_job_descriptions (3).csv\"  # Input job descriptions\n",
        "out=\"/content/resume/final_output.csv\"\n",
        "\n",
        "# Run the GNN model\n",
        "gnn_results = train_gnn_and_generate_csv(resume_data, job_data,out)\n",
        "\n",
        "# Print or save the results\n",
        "print(\"GNN Results:\")\n",
        "print(gnn_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eca4c203-cdd1-4bf4-8cf8-0edead143032",
      "metadata": {
        "id": "eca4c203-cdd1-4bf4-8cf8-0edead143032"
      },
      "outputs": [],
      "source": [
        "# Download necessary NLTK datasets\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Download the required data package\n",
        "nltk.download('punkt_tab') # This line has been added"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib"
      ],
      "metadata": {
        "id": "GScVLNRW0fwG"
      },
      "id": "GScVLNRW0fwG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d577f3d-7b8f-4a31-9cbb-91d6e644d58f",
      "metadata": {
        "id": "9d577f3d-7b8f-4a31-9cbb-91d6e644d58f"
      },
      "outputs": [],
      "source": [
        "pip install streamlit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "392a8534-55d4-4555-bd2f-c6e28761ad37",
      "metadata": {
        "id": "392a8534-55d4-4555-bd2f-c6e28761ad37"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from your_resume_parser_module import parse_resumes\n",
        "from your_portfolio_analysis_module import analyse_portfolio\n",
        "from your_career_trajectory_module import career_trajectory\n",
        "from your_gnn_module import train_gnn_and_generate_csv\n",
        "\n",
        "# Streamlit App\n",
        "st.title(\"Automated Resume Processing and Job Matching Tool\")\n",
        "\n",
        "# Step 1: Parse Resumes\n",
        "st.header(\"Step 1: Parse Resumes\")\n",
        "folder_path = st.text_input(\"Enter the folder path containing resumes:\", \"/content/resumes\")\n",
        "parsed_output = st.text_input(\"Enter output CSV file path for parsed resumes:\", \"parsed_resumes.csv\")\n",
        "\n",
        "if st.button(\"Parse Resumes\"):\n",
        "    try:\n",
        "        parse_resumes(folder_path, parsed_output)\n",
        "        st.success(f\"Resumes parsed successfully! Output saved to {parsed_output}.\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error parsing resumes: {e}\")\n",
        "\n",
        "# Step 2: Analyze GitHub Portfolio\n",
        "st.header(\"Step 2: Analyze GitHub Portfolio\")\n",
        "csv_path = st.text_input(\"Enter parsed resumes CSV file path:\", parsed_output)\n",
        "github_token = st.text_input(\"Enter your GitHub token (kept private):\", type=\"password\")\n",
        "portfolio_output = st.text_input(\"Enter output CSV file path for portfolio analysis:\", \"portfolio_analysis.csv\")\n",
        "experience_weight = st.slider(\"Experience Weight\", min_value=0.0, max_value=1.0, value=0.6)\n",
        "skills_weight = st.slider(\"Skills Weight\", min_value=0.0, max_value=1.0, value=0.4)\n",
        "\n",
        "if st.button(\"Analyze GitHub Portfolio\"):\n",
        "    try:\n",
        "        analyse_portfolio(csv_path, github_token, portfolio_output, experience_weight, skills_weight)\n",
        "        st.success(f\"Portfolio analysis completed! Output saved to {portfolio_output}.\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error analyzing portfolio: {e}\")\n",
        "\n",
        "# Step 3: Analyze Career Trajectory\n",
        "st.header(\"Step 3: Analyze Career Trajectory\")\n",
        "career_output = st.text_input(\"Enter output CSV file path for career trajectory analysis:\", \"career_trajectory_analysis.csv\")\n",
        "target_column = st.text_input(\"Enter target column for career trajectory analysis:\", \"promotions\")\n",
        "\n",
        "if st.button(\"Analyze Career Trajectory\"):\n",
        "    try:\n",
        "        career_trajectory(csv_path, target_column, career_output)\n",
        "        st.success(f\"Career trajectory analysis completed! Output saved to {career_output}.\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error analyzing career trajectory: {e}\")\n",
        "\n",
        "# Step 4: GNN Module for Resume-Job Matching\n",
        "st.header(\"Step 4: GNN Module for Resume-Job Matching\")\n",
        "resume_data = st.text_input(\"Enter processed resume data CSV file path:\", portfolio_output)\n",
        "job_data = st.text_input(\"Enter job descriptions CSV file path:\", \"/content/sample_job_descriptions.csv\")\n",
        "gnn_output = st.text_input(\"Enter output CSV file path for GNN results:\", \"final_output.csv\")\n",
        "\n",
        "if st.button(\"Run GNN and Match Resumes to Jobs\"):\n",
        "    try:\n",
        "        gnn_results = train_gnn_and_generate_csv(resume_data, job_data, gnn_output)\n",
        "        st.success(f\"GNN-based resume-job matching completed! Output saved to {gnn_output}.\")\n",
        "        st.write(\"GNN Results Preview:\")\n",
        "        st.dataframe(pd.DataFrame(gnn_results))\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error running GNN module: {e}\")\n",
        "\n",
        "# Footer\n",
        "st.write(\"### Developed by [Your Name]. Secure and efficient resume matching made simple.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7e813c8-f9d8-4dad-82f1-e47be4d6c073",
      "metadata": {
        "id": "c7e813c8-f9d8-4dad-82f1-e47be4d6c073"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21f19adf-d37f-4b52-ba63-e93572be40c7",
      "metadata": {
        "id": "21f19adf-d37f-4b52-ba63-e93572be40c7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}